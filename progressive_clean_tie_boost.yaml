# Progressive Training Config with Tie-Focused Augmentation

training:
  datasets:
    # Original clean buckets
    - name: small_512
      path: data/generated/processed_clean/small/full/train.h5
      size: 512
      curriculum_start_epoch: 1

    - name: small_512_tie_boost
      path: data/generated/processed_clean_tie_aug/small_tie/full/train.h5
      size: 512
      curriculum_start_epoch: 1

    - name: mid_1536
      path: data/generated/processed_clean/mid_1536/full/train.h5
      size: 1536
      curriculum_start_epoch: 1

    - name: mid_1536_tie_boost
      path: data/generated/processed_clean_tie_aug/mid_1536_tie/full/train.h5
      size: 1536
      curriculum_start_epoch: 1

    - name: mid_2048
      path: data/generated/processed_clean/mid_2048/full/train.h5
      size: 2048
      curriculum_start_epoch: 1

    - name: mid_2048_tie_boost
      path: data/generated/processed_clean_tie_aug/mid_2048_tie/full/train.h5
      size: 2048
      curriculum_start_epoch: 1

    - name: mid_3072
      path: data/generated/processed_clean/mid_3072/full/train.h5
      size: 3072
      curriculum_start_epoch: 2

validation:
  datasets:
    - name: val_small
      path: data/generated/processed_clean/small/full/val.h5
      size: 512

    - name: val_mid_1536
      path: data/generated/processed_clean/mid_1536/full/val.h5
      size: 1536

    - name: val_mid_2048
      path: data/generated/processed_clean/mid_2048/full/val.h5
      size: 2048

    - name: val_mid_3072
      path: data/generated/processed_clean/mid_3072/full/val.h5
      size: 3072

model:
  architecture: one_gnn
  hidden_dim: 192
  layers: 4
  dropout: 0.1
  topk: 24
  row_feat_dim: 21

training_config:
  epochs: 25
  min_epochs: 10
  patience: 6
  base_batch_size: 32
  batch_size_rules:
    - max_size: 1024
      batch_size: 32
    - max_size: 2048
      batch_size: 16
    - max_size: 4096
      batch_size: 8

  optimizer:
    type: AdamW
    lr: 0.00015
    weight_decay: 0.00005
    betas: [0.9, 0.999]

  scheduler:
    type: warmup_cosine
    warmup_ratio: 0.1
    min_lr: 1.0e-6

  loss_weights:
    primal_gap: 1.0
    feasibility: 1.0
    u_reg: 0.1
    v_reg: 0.0

  gradient_clip: 1.0
  amp_enabled: false

  curriculum:
    enabled: true
    warmup_epochs: 2

  sampling:
    strategy: round_robin
    batches_per_size: 6

checkpointing:
  save_best: true
  save_last: true
  metric: avg_primal_gap
  mode: min
  output_dir: gnn/checkpoints_clean
  checkpoint_prefix: progressive_clean_tie

logging:
  log_interval: 10
  val_log_per_split: true
