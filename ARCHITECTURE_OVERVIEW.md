# Architecture Overview - GNN-Accelerated LAP Solver Pipeline

## High-Level Architecture Flow

### Traditional LAP Pipeline
```
Cost Matrix C ‚Üí LAP Solver (Jonker-Volgenant) ‚Üí Assignment
                (cold start, full search)
```

### Our GNN-Accelerated Pipeline
```
Cost Matrix C ‚Üí GNN Model ‚Üí Dual Potentials (u, v) ‚Üí Seeded LAP ‚Üí Assignment
                (learned)    (warm start)              (partial search)
```

---

## üéØ Core Innovation: Warm-Starting with Learned Dual Potentials

### The Problem
Classical LAP solvers (Jonker-Volgenant algorithm) start with **zero dual potentials** and must explore the full search space. For large problems (n > 1000), this becomes computationally expensive.

### Our Solution
We use **Graph Neural Networks (GNNs)** to predict high-quality dual potentials from the cost matrix, then use these as a **warm-start** for a modified LAP solver. This dramatically reduces the search space.

---

## üìä Pipeline Components

### 1. **Feature Engineering** (`gnn/features.py`)
- **Input**: Cost matrix C (n√ón)
- **Output**: Row descriptors (and optional column/edge descriptors for DualGNN)
- **Process**: Extract statistical and positional signals from the cost matrix:
  - Row/column statistics (min, max, mean, std, entropy)
  - Competition metrics (second-best gap, near-best ratio)
  - Positional encodings (sinusoidal)
  - Rank normalization and normalized competition

These summaries compress the dense n¬≤ matrix into structured features that feed the networks without scanning every edge during inference.

### 2. **GNN Models** (Two Architectures)

#### **DualGNN** (`gnn/dual_gnn.py`) - Predict Both Duals
- **Goal**: Produce `u` and a direct hint for `v` in one pass.
- **Architecture**: Graph attention stack that exchanges messages along every edge, so it captures the full bipartite structure and learns tight dual pairs.
- **Trade-off**: Strong accuracy but quadratic runtime because it materializes edge features. Useful when we can pay for longer inference or want a gold standard.

#### **OneGNN** (`gnn/one_gnn.py`) - Predict Rows Only
- **Goal**: Retain most of the speedup while eliminating explicit edge processing.
- **Architecture**: Per-row MLP with residual refinement plus a top-k attention layer that only inspects the cheapest columns per row.
- **Why we chose it**: Benchmarks showed the seeded solver mainly needs accurate `u`. Once `u` is known, we recover `v` cheaply via `v_j = min_i(C_{ij} - u_i)`. Dropping n¬≤ attention cut inference latency by ~10x on large problems and made GPU batching practical.
- **How we made it work**:
  - Rich row statistics supply indirect information about column competition.
  - The sparse top-k refinement injects just enough structural signal for hard instances.
  - Training emphasizes dual feasibility so the projected `v` stays valid after the min-trick.

### 3. **Custom Seeded LAP Solver** (`LAP/lap/lapjv_seeded`)

**Key Difference from Original LAP**: The original LAP library only supports **cold-start** (zero initialization). We extended it to accept **warm-start** dual potentials.

#### Original LAP (lapjv):
```cpp
lapjv(C) {
    u = zeros(n)  // cold start
    v = zeros(n)
    // full Jonker-Volgenant search
}
```

#### Our Seeded LAP (lapjv_seeded):
```cpp
lapjv_seeded(C, u_seed, v_seed) {
    u = u_seed  // warm start from GNN
    v = v_seed
    
    // OPTIMIZATION 1: Feasibility projection
    project_to_feasible_region(u, v, C)
    
    // OPTIMIZATION 2: Row tightening
    for each row i: u[i] = min_j(C[i,j] - v[j])
    
    // OPTIMIZATION 3: Greedy matching on tight edges
    match_zero_reduced_cost_edges()
    
    // OPTIMIZATION 4: Micro augmenting row reduction
    create_second_zeros_for_unmatched()
    
    // OPTIMIZATION 5: Fallback to full JV if seed quality low
    if (tight_edge_density < 1.2n)
        return lapjv_internal(C)  // full search
    
    // Only run augmenting paths on remaining unmatched rows
    augment_remaining(free_rows)
}
```

**Key Optimizations**:
1. **Feasibility Projection**: Ensures u[i] + v[j] ‚â§ C[i,j] (dual feasibility)
2. **Row Tightening**: Creates at least one zero per row in reduced cost matrix
3. **Greedy Matching**: Matches as many rows as possible on tight (zero) edges
4. **Micro-ARR**: Creates second zeros for unmatched rows to improve augmentation
5. **Quality-Based Fallback**: Falls back to full search if seed is poor

---

## üîÑ End-to-End Pipeline Flow

### Phase 1: Training (Offline)
```
1. Generate synthetic LAP instances with known optimal solutions
   ‚îú‚îÄ‚îÄ Use SciPy to solve and extract oracle dual potentials
   ‚îî‚îÄ‚îÄ Store: (Cost Matrix, u*, v*) ‚Üí HDF5 datasets

2. Train GNN Model
   ‚îú‚îÄ‚îÄ Input: Cost matrix features
   ‚îú‚îÄ‚îÄ Target: Oracle dual potentials u*, v*
   ‚îú‚îÄ‚îÄ Loss: Primal gap + Feasibility violation + MSE
   ‚îî‚îÄ‚îÄ Output: Trained GNN checkpoint

3. Validate Model
   ‚îú‚îÄ‚îÄ Test on held-out instances
   ‚îú‚îÄ‚îÄ Measure: Dual prediction accuracy, speedup vs baseline
   ‚îî‚îÄ‚îÄ Select best checkpoint
```

### Phase 2: Inference (Online)
```
1. New Problem Arrives
   ‚îî‚îÄ‚îÄ Cost Matrix C (n√ón)

2. Feature Extraction
   ‚îú‚îÄ‚îÄ Compute row features
   ‚îî‚îÄ‚îÄ Optional: column/edge features for DualGNN

3. GNN Inference (GPU/CPU)
   ‚îú‚îÄ‚îÄ Forward pass through trained model
   ‚îú‚îÄ‚îÄ Output:
       - DualGNN ‚Üí direct (u_pred, v_hint)
       - OneGNN ‚Üí row duals u_pred only
   ‚îî‚îÄ‚îÄ Time: ~1-10ms for n=2048 (GPU)

4. Seeded LAP Solve
   ‚îú‚îÄ‚îÄ Input: (C, u_pred, computed v_pred)
   ‚îú‚îÄ‚îÄ Warm-start optimization
   ‚îî‚îÄ‚îÄ Output: Optimal assignment
   ‚îî‚îÄ‚îÄ Time: 2-5x faster than cold start

5. Return Assignment
```

---

## üìà Performance Characteristics

### Speedup Breakdown
- **GNN Inference**: 1-10ms (GPU), 10-50ms (CPU)
- **Seeded LAP**: 40-60% faster than cold LAP
- **Overall Pipeline**: 2-5x speedup on large problems (n > 1024)

### When It Works Best
- **Large problems** (n > 1024): More room for optimization
- **Similar problem distributions**: GNN learns problem structure
- **Repeated solves**: Training cost amortized over many instances

### Limitations
- **Training overhead**: Requires dataset generation
- **Distribution shift**: Performance degrades on out-of-distribution problems
- **Small problems** (n < 256): GNN overhead dominates savings

---

## üî¨ Technical Innovations

### 1. **One Prediction Strategy with Min-Trick**
**Key Innovation**: Predict only row duals u, derive column duals v mathematically
- **Min-trick**: v_j = min_i(C_ij - u_i) ensures dual feasibility
- **Sparse refinement**: Top-k aggregation adds global context in O(nk)
- **Result**: Linear model complexity with quadratic problem understanding

### 2. **Dual Prediction Loss Function**
```python
Loss = primal_gap         # Ensure dual lower bound quality
     + feasibility_term   # Ensure u + v ‚â§ C
     + MSE(u_pred, u*)   # Match oracle duals
```
For OneGNN: Supervise u, but loss depends on derived v ‚Üí model learns to predict u that yields good v

### 3. **Adaptive Seeded Solver**
- Automatically detects poor seeds and falls back
- Balances greedy matching vs. augmenting paths
- Maintains optimality guarantees

### 4. **Two Prediction Strategies**
- **Dual Prediction (DualGNN)**: Predict u and v independently - maximum accuracy
- **One Prediction (OneGNN)**: Predict u, derive v - maximum speed and scalability

---

---

## üéì Quick Comparison: Dual vs One Prediction

| Aspect | Dual Prediction (DualGNN) | One Prediction (OneGNN) |
|--------|---------------------------|-------------------------|
| **What to predict** | Both u AND v | Only u |
| **How to get v** | Direct prediction | Derived: v = min(C - u) |
| **Model complexity** | O(n¬≤) edge processing | O(n) row processing |
| **Inference time** | Slower (~10x) | Faster |
| **Accuracy** | Maximum | Near-maximum |
| **Scalability** | Limited (n < 2048) | Excellent (n > 2048) |
| **Use case** | Research, validation | Production, large-scale |

**Bottom line**: OneGNN achieves 90% of DualGNN's accuracy at 10% of the computational cost by exploiting the mathematical structure of dual feasibility.

---

## üìù Summary for Instructor

**Our architecture transforms the Linear Assignment Problem solving from a pure algorithmic approach to a hybrid learning + optimization pipeline:**

1. **We extended the classical LAP solver** to accept warm-start dual potentials (lapjv_seeded), implementing 5 key optimizations that exploit good initial solutions.

2. **We explored two prediction strategies** for learning dual potentials:
   - **Dual Prediction**: Predict both u and v (high accuracy, O(n¬≤) cost)
   - **One Prediction**: Predict only u, derive v via min-trick (scalable, O(n) model)

3. **One Prediction breakthrough** - We made it work through:
   - Mathematical foundation: v_j = min_i(C_ij - u_i) guarantees dual feasibility
   - Sparse top-k refinement: O(nk) global context without full edge processing
   - Loss design: Supervise u but optimize derived (u,v) pair quality
   - Result: Linear complexity with near-dual-prediction accuracy

4. **The pipeline achieves 2-5x speedups** on large problems by:
   - Learning problem structure from data
   - Reducing search space via warm-starting
   - Maintaining optimality guarantees through fallback mechanisms

5. **The system is production-ready** with:
   - Two models: DualGNN (accuracy) and OneGNN (speed/scale)
   - Modular architecture (solvers/, gnn/, data/)
   - Comprehensive logging and experiment tracking
   - GPU acceleration support

**Key Insights**: 
- Dual potentials can be learned from cost matrix features
- One prediction with min-trick achieves scalability without sacrificing quality
- Hybrid learning + optimization outperforms pure algorithmic approaches

---

## üöÄ Current Status

- ‚úÖ Two trained GNN architectures (DualGNN, OneGNN)
- ‚úÖ Custom seeded LAP solver with 5 optimizations
- ‚úÖ Comprehensive benchmarking suite
- ‚úÖ Multiple model checkpoints for different problem sizes
- ‚úÖ Full training/evaluation pipeline
- ‚úÖ GPU acceleration support
- üìä Validated on synthetic datasets (uniform, clustered, metric)
- üéØ Ready for real-world problem deployment
